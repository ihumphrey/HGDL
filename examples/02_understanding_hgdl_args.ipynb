{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understading HGDL's Arguments\n",
    "First, let's look at its docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgdl.hgdl import HGDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mHGDL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "HGDL\n",
       "    * Hybrid - uses both local and global optimization\n",
       "    * G - uses global optimizer\n",
       "    * D - uses deflation\n",
       "    * L - uses local extremum localMethod\n",
       "    Mandatory Parameters:\n",
       "        * func - should return a scalar given a numpy array x\n",
       "        -- note: use functools.partial if you have optional params\n",
       "        * grad - gradient vector at x\n",
       "        * bounds - numpy array of bounds in same format as scipy.optimize\n",
       "    Optional Parameters:\n",
       "        * Overall Parameters -----------------------------------\n",
       "        * hess (None) - hessian array at x - you may need this depending on the local method\n",
       "        * client - (None->HGDL initializes if None) dask.distributed.Client object\n",
       "            -- this lets you interface with clusters via dask with Client(myCluster)\n",
       "        * num_epochs (10) - the number of epochs. 1 epoch is 1 global step + 1 local run\n",
       "        * numWorkers (logical cpu cores -1) - how many processes to use\n",
       "        * fix_rng (True) - sets random numbers to be fixed (for reproducibility)\n",
       "        * bestX (5) - maximum number of minima and global results to put in get_final()\n",
       "        * num_individuals (25) - the number of individuals to run for both global and local methods\n",
       "        * x0 (None) starting points to probe\n",
       "        * global_method ('genetic') - these control what global and local methods\n",
       "        * local_method ('my_newton') -    are used by HGDL\n",
       "\n",
       "        * Global Method Parameters -----------------------------\n",
       "        * global_args ((,)) - arguments to global method\n",
       "        * global_kwargs({}) - kwargs for global method\n",
       "            -- note: these let you pass custom info to your method of choice\n",
       "\n",
       "        * Deflation Parameters ---------------------------------\n",
       "        * alpha (0.1) - the alpha term of the bump function\n",
       "        * r (.3) - the radius for the bump function\n",
       "            -- these define how deflation behaves\n",
       "\n",
       "        * Local Method Parameters ------------------------------\n",
       "        * local_args ((,)) - arguments to global method\n",
       "        * local_kwargs({}) - kwargs for global method\n",
       "            -- note: these let you pass custom info to your method of choice\n",
       "        * max_local (5) - the maximum number of local runs to do\n",
       "\n",
       "    Returns:\n",
       "        an HGDL object that has the following functions:\n",
       "        get_best(): yields a dict of the form\n",
       "            {\"best_x\":best_x_ndarray, \"best_y\":best_y_value}\n",
       "        get_final(): yields a dict of the form\n",
       "            {\"best_x\":best_x_ndarray, \"best_y\":best_y_value,\n",
       "            \"minima_x\":minima_x_ndarray, \"minima_y\":minima_y_values,\n",
       "            \"global_x\":global_x_ndarray, \"global_y\":global_y_values,\n",
       "            }\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.7/site-packages/hgdl/hgdl.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HGDL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overall Parameters -----------------------------------\n",
    "       * These are parameters that control the overall behavior\n",
    "    * hess (None) - hessian array at x - you may need this depending on the local method\n",
    "        * This has the same format as grad in the mandatory arguments. This should return the \n",
    "            hessian at a single point x\n",
    "    * client - (None->HGDL initializes if None) dask.distributed.Client object\n",
    "        * on a large computer, you could want to do something similar to \n",
    "            https://docs.dask.org/en/latest/setup/hpc.html\n",
    "    * num_epochs (10) - the number of epochs. 1 epoch is 1 global step + 1 local run\n",
    "        * This sets the number of epochs run. There are a few layers HGDL:\n",
    "            - for each epoch\n",
    "                - global step: \n",
    "                    for num_individuals:\n",
    "                        calculate new points to probe and evaluate func at each\n",
    "                - local step:\n",
    "                    for max_local steps\n",
    "                        for num_individuals:\n",
    "                            run local method\n",
    "    * fix_rng (True) - sets random numbers to be fixed (for reproducibility)\n",
    "        * HGDL provides a random number generator to all methods. This is seeded with 42 unless\n",
    "            told otherwise. the reason for this is for replicating the code in our paper\n",
    "    * bestX (5) - maximum number of minima and global results to put in get_final()\n",
    "        * This is the maximum length of minima_x and global_x in the dictionary returned by \n",
    "            get_final(). set to -1 for all. \n",
    "    * num_individuals (25) - the number of individuals to run for both global and local methods\n",
    "    * x0 (None) starting points to probe\n",
    "        * this should be starting point in the form (num_samples, dimension_of_space)\n",
    "    * global_method ('genetic') - these control what global and local methods\n",
    "        * I have build in 'genetic' and Marcus's 'gaussian'. to add more, just look in the\n",
    "            git repo under global, and add to run_global in the same style as exists\n",
    "    * local_method ('my_newton') -    are used by HGDL\n",
    "        * same but for local methods. i have 'scipy' and 'my_newton', but again, you can just\n",
    "            add to the local folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Global Method Parameters -----------------------------\n",
    "    * global_args ((,)) - arguments to global method\n",
    "    * global_kwargs({}) - kwargs for global method\n",
    "        - note: these let you pass custom info to your method of choice\n",
    "        - When you make a custom global method, you could have additional parameters or kwargs. these give you a way of storing them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deflation Parameters ---------------------------------\n",
    "    * alpha (0.1) - the alpha term of the bump function\n",
    "        * alpha determines the shape of the bump function used to \n",
    "            deflate the minima\n",
    "    * r (.3) - the radius for the bump function\n",
    "        * r determines how far away something is for it to be in\n",
    "            range of deflation. it is the range of the bump function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Local Method Parameters ------------------------------\n",
    "    * local_args ((,)) - arguments to global method\n",
    "    * local_kwargs({}) - kwargs for global method\n",
    "        -- note: these let you pass custom info to your method of choice\n",
    "        * same as with global_args and global_kwargs\n",
    "            with the 'scipy' option you can pass options to scipy\n",
    "            such as tolerances\n",
    "    * max_local (5) - the maximum number of local runs to do\n",
    "        * the local method is run max_local*num_individual times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
