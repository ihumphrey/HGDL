{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGDL\n",
    "    * Hybrid - uses both local and global optimization\n",
    "    * G - uses global optimizer\n",
    "    * D - uses deflation\n",
    "    * L - uses local extremum method\n",
    "The goal of this is to be modular and robust to a variety of functions\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, these are all necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from math import ceil\n",
    "import numba as nb\n",
    "from multiprocessing import cpu_count\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization\n",
    "### Parameters - passing a bunch of params is unseemly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultParams():\n",
    "    parameters = np.dtype([('numWorkers','i2'),\n",
    "                            ('epsilon','f8'),\n",
    "                            ('radius_squared','f8'),\n",
    "                            ('maxCount','i4'),\n",
    "                            ('alpha','f8'),\n",
    "                            ('unfairness','f8'),\n",
    "                            ('wildness','f8'),\n",
    "                            ('minImprovement','f8'),\n",
    "                            ('N','i4'),\n",
    "                            ('keepLastX','i2'),\n",
    "                            ('maxRuns','i4'),\n",
    "                            ('returnedThreshold','f8'),\n",
    "                            ('verbose','b'),\n",
    "                            ('k','i4'),\n",
    "                            ('numGenerations','i4'),\n",
    "                          ])\n",
    "\n",
    "    parameters = np.recarray(1, parameters)\n",
    "    \n",
    "    parameters.numWorkers = -1\n",
    "    parameters.epsilon = 1e-2\n",
    "    parameters.maxCount = 100\n",
    "    parameters.alpha = .1\n",
    "    parameters.unfairness = 2.5\n",
    "    parameters.wildness = 1\n",
    "    parameters.minImprovement = 1.1 # I don't think I am using this right now\n",
    "    parameters.N = 100\n",
    "    parameters.keepLastX = 10\n",
    "    parameters.maxRuns = 10\n",
    "    parameters.returnedThreshold=0.7\n",
    "    parameters.verbose = False\n",
    "    parameters.numGenerations = 0\n",
    "    \n",
    "    parameters.k = -2\n",
    "    parameters.radius_squared = -2.\n",
    "    \n",
    "    return parameters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define deflation operator and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.vectorize(nopython=True, cache=True)\n",
    "def bump_function(dist2center, radius_squared, alpha):\n",
    "    \"\"\"\n",
    "    This actually takes the squared distances to the center, |x-x0|^2\n",
    "    This is vectorized over distances\n",
    "    Marcus's bump function\n",
    "        - find it at https://www.sciencedirect.com/science/article/pii/S037704271730225X\n",
    "    \"\"\"\n",
    "    if dist2center==radius_squared: return 0\n",
    "    #if dist2center<1e-7: return 10000\n",
    "    bump_term = np.exp( (-alpha)/(radius_squared - dist2center) + (alpha/radius_squared) )\n",
    "    return 1./(1.-bump_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    x = np.arange(0, 2.5, 1e-2)\n",
    "    plt.plot(x, bump_function(x, 2.5, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.vectorize(nopython=True, cache=True)\n",
    "def bump_derivative(dist2center, radius_squared, alpha):\n",
    "    \"\"\"\n",
    "    This actually takes the squared distances to the center, |x-x0|^2\n",
    "    This is vectorized over distances\n",
    "    Marcus's bump function\n",
    "        - find it at https://www.sciencedirect.com/science/article/pii/S037704271730225X\n",
    "    \"\"\"\n",
    "    if dist2center==radius_squared: return 0\n",
    "    bump_der = np.exp( (-alpha)/(radius_squared - dist2center) + (alpha/radius_squared) )\n",
    "    bump_der *= -2*alpha*np.sqrt(dist2center)/np.power(radius_squared-dist2center,2)\n",
    "    return np.power(bump_function(dist2center,radius_squared,alpha),2)*bump_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    x = np.arange(0, 2.4, 1e-2)\n",
    "    plt.plot(x, bump_derivative(x, 2.5, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, cache=True)\n",
    "def deflation_factor(x, minima, radius_squared, alpha):\n",
    "    \"\"\"\n",
    "    This calculates:\n",
    "        * what minima is this x in range of\n",
    "        * for the minima in range, what is their deflation factor\n",
    "        * combined defaltion factor\n",
    "    \"\"\"\n",
    "    # initialize scaling factor\n",
    "    factor = 1.\n",
    "    xLen = len(x)\n",
    "    zLen = len(minima)\n",
    "    \n",
    "    # doing all the math in matrix form is much faster \n",
    "    c = x-minima[:,:xLen]\n",
    "    dists2center = np.sum(c*c,axis=1)\n",
    "    withinRange = dists2center < radius_squared\n",
    "    return np.prod(bump_function(dists2center[withinRange], radius_squared, alpha)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    assert np.isinf(deflation_factor(np.ones(3), np.ones((1,3)), 1., 1.,)), \"deflation factor is not blowing up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    for i in range(100):\n",
    "        stop = np.random.random()*100\n",
    "        alpha = np.random.random()*3 + .1\n",
    "        x = np.arange(0, stop, 1e-3)\n",
    "        y = bump_function(x, stop, alpha)\n",
    "        assert np.array([y[i]<=y[i-1] for i in range(1,len(y))]).all(), \"bump function is not monotonically decreasing for alpha \"+str(alpha)+' and stop: '+str(stop)\n",
    "        y = bump_derivative(x, stop, alpha)\n",
    "        assert np.array([y[i]>=y[i-1] for i in range(2,len(y))]).all(), \"bump derivative is not monotonically increasing for alpha \"+str(alpha)+' and stop: '+str(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, cache=True)\n",
    "def deflation_derivative(x, minima, radius_squared, alpha):\n",
    "    \"\"\"\n",
    "    This calculates:\n",
    "        * what minima is this x in range of\n",
    "        * for the minima in range, what is their deflation factor\n",
    "        * combined defaltion factor\n",
    "    \"\"\"\n",
    "    # initialize scaling factor\n",
    "    factor = 1.\n",
    "    xLen = len(x)\n",
    "    zLen = len(minima)\n",
    "    \n",
    "    # doing all the math in matrix form is much faster \n",
    "    c = x-minima[:,:xLen]\n",
    "    dists2center = np.sum(c*c,axis=1)\n",
    "    withinRange = dists2center < radius_squared\n",
    "    return np.prod(bump_derivative(dists2center[withinRange], radius_squared, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    assert np.isinf(deflation_derivative(np.ones(3), np.ones((1,3))+1e-10, 1., 1.,)), \"deflation derivative is not blowing up\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define checks (necessary bc of parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, cache=True)\n",
    "def alreadyFound(newMinima, oldMinima, radius_squared, k):\n",
    "    \"\"\"\n",
    "    check if the new minimum is within range of the old ones\n",
    "    \"\"\"\n",
    "    c = oldMinima[:,:k] - newMinima[0,:k]\n",
    "    return (np.sum(c*c,1)<radius_squared).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    assert alreadyFound(10*np.ones((1, 4)), 10*np.ones((1,4)), 10, 3), \"not noticing already found minima\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define wrappers to make interface generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflated_gradient(x, gradient, minima, radius_squared, alpha):\n",
    "    \"\"\"\n",
    "    This just combines these two functions together\n",
    "    \"\"\"\n",
    "    return gradient(x)*deflation_factor(x, minima, radius_squared, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflated_hessian(x, gradient, hessian, minima, radius_squared, alpha):\n",
    "    \"\"\"\n",
    "    This just combines these two functions together\n",
    "    \"\"\"\n",
    "    term1 = hessian(x)*deflation_derivative(x, minima, radius_squared, alpha)\n",
    "    term2 = gradient(x)*deflation_factor(x, minima, radius_squared, alpha)\n",
    "    return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_wrapper(x0, objective):\n",
    "    \"\"\"\n",
    "    The partial function requires x to be in front, so this just does\n",
    "    a switcheroo with w(x,f,..) <=> w(f,x)\n",
    "    \"\"\"\n",
    "    return minimize(fun=objective, x0=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    A = 10\n",
    "    d = 2\n",
    "    def Rastringin(x):\n",
    "        if args is not (): print(*args)\n",
    "        return (A*d + np.dot(x,x) - A*np.sum(np.cos(2.*np.pi*x)))\n",
    "\n",
    "    def Rastringin_gradient(x):\n",
    "        if args is not (): print(*args)\n",
    "        grad = np.empty(len(x))\n",
    "        for i in range(len(grad)):\n",
    "            grad[i] = (2.*x[i] + A*np.sin(2.*np.pi*x[i])*2.*np.pi)\n",
    "        return grad\n",
    "\n",
    "    def Rastringin_hessian(x):\n",
    "        if args is not (): print(*args)\n",
    "        hess = np.zeros((len(x),len(x)))\n",
    "        for i in range(len(hess)):\n",
    "            hess[i,i] = (2 + A*np.cos(2.*np.pi*x[i])*(4.*np.pi*np.pi))\n",
    "        return hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parallelized deflated local stepdefaultParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizer(x, objective, method, jac, hess, tol, options):\n",
    "    return minimize(objective, x, method=method, jac=jac, hess=hess, tol=tol, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(x, objective, **kwargs):\n",
    "    return minimize(objective, x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_individuals(individuals, bounds, objective, gradient, Hessian, workers, parameters, method, minima=None):\n",
    "    \"\"\"\n",
    "    Do the actual iteration through the deflated local optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    if minima is None: minima = np.empty((0, parameters.k+1))\n",
    "    \n",
    "    newMinima = np.empty((0, parameters.k+1))\n",
    "        \n",
    "    for i in range(parameters.maxRuns):\n",
    "        numNone = 0\n",
    "        \n",
    "        # construct the jacobian, hessian, and objective. This is inside the loop because of the minima\n",
    "        jac = partial(deflated_gradient, gradient=gradient, minima=minima, radius_squared=parameters.radius_squared, \n",
    "                      alpha=parameters.alpha)\n",
    "        \n",
    "        if Hessian is not None: Hessian = partial(deflated_hessian, gradient=gradient, hessian=Hessian, minima=minima,\n",
    "                                                  radius_squared=parameters.radius_squared, alpha=parameters.alpha)\n",
    "        \n",
    "        minimizer = partial(wrapper, objective=objective, method=method, jac=jac, hess=Hessian, \n",
    "                    tol=parameters.radius_squared, options={'maxiter':parameters.maxCount})\n",
    "\n",
    "        # chunk up and iterate through\n",
    "        walkerOutput = workers.imap_unordered(minimizer, individuals, chunksize=ceil(0.3*len(individuals)/cpu_count()))\n",
    "\n",
    "        # process results\n",
    "        for i,x_found in enumerate(walkerOutput):\n",
    "            if x_found.success==False: \n",
    "                numNone += 1\n",
    "                if numNone/parameters.N > parameters.returnedThreshold:\n",
    "                    return minima\n",
    "            else:\n",
    "                if not in_bounds(x_found.x, bounds): \n",
    "                    numNone += 1\n",
    "                    if numNone/parameters.N > parameters.returnedThreshold:\n",
    "                        return minima                        \n",
    "                else:\n",
    "                    newMinima = np.array([*x_found.x, x_found.fun]).reshape(1,-1)\n",
    "                    if len(minima)==0:\n",
    "                        minima = np.concatenate((newMinima, minima), axis=0)\n",
    "                    elif not alreadyFound(newMinima, minima, radius_squared=parameters.radius_squared, k=parameters.k): \n",
    "                        minima = np.concatenate((newMinima, minima), axis=0)\n",
    "    return minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a global optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Procreate(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    X is the individuals - points on a surface\n",
    "    y is the performance - f(X)\n",
    "    \n",
    "    Parameters:\n",
    "    unfairness is a metric from (1,infinity), where higher numbers skew more towards well performing individuals having more kids\n",
    "    cauchy wildness is a scaling factor for the standard cauchy distribution, where higher values mean less variation\n",
    "        (although it's still a nonstationary function...)\n",
    "    \n",
    "    Notes:\n",
    "    some of the children can (and will) be outside of the bounds!\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize the performances to (0,1)\n",
    "    p = y - np.amin(y)\n",
    "    maxVal = np.amax(p)\n",
    "    if maxVal == 0: p = np.ones(len(p))/len(p) # if the distribution of performance has no width, give everyone an equal shot\n",
    "    else: p /= maxVal\n",
    "\n",
    "    #This chooses from the sample based on the power law, allowing replacement means that the the individuals can have multiple kids\n",
    "    p = parameters.unfairness*np.power(p,parameters.unfairness-1)\n",
    "    p /= np.sum(p)\n",
    "    if np.isnan(p).any():\n",
    "        return p, X, y\n",
    "    moms = np.random.choice(np.arange(len(p)), parameters.N, replace=True, p = p)\n",
    "    dads = np.random.choice(np.arange(len(p)), parameters.N, replace=True, p = p)\n",
    "    \n",
    "    # calculate a perturbation to the median of each individuals parents\n",
    "    perturbation = np.random.standard_normal(size=(parameters.N,parameters.k))\n",
    "    perturbation *= parameters.wildness\n",
    "    \n",
    "    # the children are the median of their parents plus a perturbation (with a chance to deviate wildly)\n",
    "    children = (X[moms]+X[dads])/2. + perturbation*(X[moms]-X[dads])\n",
    "\n",
    "    return children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check boundary conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_bounds(x, bounds):\n",
    "    if (bounds[:,1]-x > 0).all() and (bounds[:,0] - x < 0).all(): return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample within bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(N,bounds,parameters):\n",
    "    sample = np.random.random((N,parameters.k))\n",
    "    sample *= bounds[:,1]-bounds[:,0]\n",
    "    sample += bounds[:,0]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HXDY(fun, bounds, jac, method=None, hess=None, x0=None, \n",
    "         parameters=None, rms = .01, extraStoppingCriterion=None ):\n",
    "    \n",
    "    # Initialization\n",
    "    if parameters == None:\n",
    "        parameters = defaultParams()\n",
    "\n",
    "    parameters.k = len(bounds)\n",
    " \n",
    "    if parameters.numWorkers == -1: parameters.numWorkers = cpu_count()\n",
    "\n",
    "    if x0 is None: starts = random_sample(parameters.N,bounds,parameters)\n",
    "    else: starts = x0\n",
    "        \n",
    "    objective = fun\n",
    "    hessian = hess\n",
    "    gradient = jac\n",
    "    parameters.radius_squared = parameters.k*(rms**2)\n",
    "    \n",
    "    if extraStoppingCriterion is None: extraStoppingCriterion = lambda x: True\n",
    "        \n",
    "    workers = Pool(parameters.numWorkers)\n",
    "     \n",
    "    res = walk_individuals(starts, bounds, objective, gradient, hessian, workers, parameters, method=method)\n",
    "    \n",
    "    parameters.numGenerations = 0\n",
    "    \n",
    "    # processing\n",
    "    res = res[res[:,-1].argsort()]\n",
    "    best = np.inf*np.ones(parameters.keepLastX); \n",
    "    if len(res)==0: best[0] = np.nan_to_num(np.inf)-1\n",
    "    else: best[0] = res[0,-1]\n",
    "        \n",
    "    while not np.allclose(best[0],best) and extraStoppingCriterion(res):\n",
    "\n",
    "        parameters.numGenerations += 1\n",
    "\n",
    "        if res.shape[0]!=0: \n",
    "            new_starts = Procreate(res[:,:parameters.k], res[:,-1], parameters)\n",
    "            for i in range(len(new_starts)): \n",
    "                if not in_bounds(new_starts[i], bounds): new_starts[i] = random_sample(1,bounds,parameters)\n",
    "        else:\n",
    "            new_starts = random_sample(parameters.N,bounds,parameters)\n",
    "\n",
    "        res = walk_individuals(new_starts, bounds, objective, gradient, hessian, workers, parameters, minima=res, method=method)\n",
    "        \n",
    "        res = res[res[:,-1].argsort()]\n",
    "        if res.shape[0]!=0: best[parameters.numGenerations%parameters.keepLastX] = res[0,-1]\n",
    "        if parameters.verbose: print(best.round())\n",
    "        \n",
    "    workers.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
